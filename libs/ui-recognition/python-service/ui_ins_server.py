#!/usr/bin/env python3
"""
çœŸæ­£çš„UI-Ins-7BæœåŠ¡
åŸºäºé˜¿é‡Œé€šä¹‰UI-Ins-7Bæ¨¡å‹çš„UIè¯†åˆ«æœåŠ¡
"""

import base64
import json
import time
import re
import io
import os
import argparse
from typing import Dict, List, Any, Optional, Tuple
from PIL import Image
import numpy as np
import torch
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UIIns7BService:
    def __init__(self, model_path: str = "./models/ui-ins-7b"):
        self.model_path = model_path
        self.model = None
        self.processor = None
        self.model_loaded = False

    def load_model(self):
        """åŠ è½½UI-Ins-7Bæ¨¡å‹"""
        try:
            logger.info("å¼€å§‹åŠ è½½UI-Ins-7Bæ¨¡å‹...")

            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")

            # æ£€æŸ¥å¿…è¦æ–‡ä»¶
            required_files = ['config.json', 'tokenizer.json']
            for file in required_files:
                if not os.path.exists(os.path.join(self.model_path, file)):
                    raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file}")

            logger.info("âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")

            # åŠ è½½å¤„ç†å™¨
            logger.info("åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                local_files_only=True
            )
            logger.info("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")

            # åŠ è½½æ¨¡å‹ - ä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°
            logger.info("åŠ è½½æ¨¡å‹æƒé‡...")
            device = "mps" if torch.backends.mps.is_available() else "cpu"
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

            if device == "mps":
                logger.info("âš ï¸ MPSè®¾å¤‡ä¸æ”¯æŒbfloat16ï¼Œä½¿ç”¨float32")
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    device_map="auto",
                    trust_remote_code=True,
                    local_files_only=True,
                    low_cpu_mem_usage=True
                ).eval()
            else:
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True,
                    local_files_only=True,
                    low_cpu_mem_usage=True
                ).eval()

            self.model_loaded = True
            logger.info("âœ… UI-Ins-7Bæ¨¡å‹åŠ è½½å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ UI-Ins-7Bæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"UI-Ins-7Bæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    def _decode_image(self, image_data: str) -> Image.Image:
        """è§£ç base64å›¾åƒ"""
        try:
            # ç§»é™¤data URLå‰ç¼€
            if image_data.startswith('data:image'):
                image_data = image_data.split(',')[1]

            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes))

            # è½¬æ¢ä¸ºRGBæ ¼å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')

            return image

        except Exception as e:
            raise ValueError(f"æ— æ³•è§£ç å›¾åƒ: {e}")

    def _parse_coordinates(self, response_text: str) -> Tuple[int, int]:
        """ä»å“åº”ä¸­è§£æåæ ‡ - ä½¿ç”¨UI-Ins-7Bå®˜æ–¹æ ¼å¼"""
        try:
            # æŸ¥æ‰¾JSONæ ¼å¼çš„function_call
            if '<function_call>' in response_text and '</function_call>' in response_text:
                func_start = response_text.find('<function_call>') + len('<function_call>')
                func_end = response_text.find('</function_call>')
                func_content = response_text[func_start:func_end].strip()

                # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
                func_content = func_content.replace('```json', '').replace('```', '').strip()

                func_data = json.loads(func_content)
                if 'arguments' in func_data and 'coordinate' in func_data['arguments']:
                    coordinate = func_data['arguments']['coordinate']
                    if isinstance(coordinate, list) and len(coordinate) >= 2:
                        return coordinate[0], coordinate[1]

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°function_callï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾åæ ‡
            coord_match = re.search(r'\[(\d+)\s*,\s*(\d+)\]', response_text)
            if coord_match:
                return map(int, coord_match.groups())

            return -1, -1

        except Exception as e:
            logger.warning(f"åæ ‡è§£æå¤±è´¥: {e}")
            return -1, -1

    def recognize_ui_elements(self, image: Image.Image, query: str) -> Dict[str, Any]:
        """ä½¿ç”¨UI-Ins-7Bè¯†åˆ«UIå…ƒç´ """
        if not self.model_loaded:
            self.load_model()

        try:
            start_time = time.time()
            logger.info(f"æ‰§è¡ŒUI-Ins-7Bæ¨ç†: {query}")

            # æ„å»ºUI-Ins-7Bå®˜æ–¹GUI agentæ ¼å¼
            # ä¸å®˜æ–¹ç¤ºä¾‹ä¿æŒä¸€è‡´çš„ç³»ç»Ÿæ¶ˆæ¯ä¸æ ¼å¼è¯´æ˜
            sys_msg_1 = "You are a helpful assistant."
            sys_msg_2 = (
                "You are a GUI agent. You are given a task and your action history, with screenshots. "
                "You need to perform the next action to complete the task.\n\n"
                "## Output Format\n"
                "Return a json object with a reasoning process in <reasoning> tags, a function name and arguments within <function_call> XML tags:\n```
<reasoning>\n...\n</reasoning>\n\n\n"
                "<function_call>\n{\"name\": \"grounding\", \"arguments\": {\"action\": \"click\", \"coordinate\": [x, y]}}\n</function_call>\n```
 represents the following item of the action space:\n"
                "## Action Space{\"action\": \"click\", \"coordinate\": [x, y]}\n"
                "Your task is to accurately locate a UI element based on the instruction. "
                "You should first analyze instruction in <reasoning> tags and finally output the function in <function_call> tags."
            )

            messages = [
                {
                    "role": "system",
                    "content": [
                        {"type": "text", "text": sys_msg_1},
                        {"type": "text", "text": sys_msg_2}
                    ]
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": query}
                    ]
                }
            ]

            # åº”ç”¨èŠå¤©æ¨¡æ¿
            prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[prompt], images=[image], return_tensors="pt"
            ).to(self.model.device)

            # ç”Ÿæˆå“åº”
            # å®˜æ–¹ç¤ºä¾‹é»˜è®¤ç”Ÿæˆé•¿åº¦
            max_tokens = 128
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=False,
                    temperature=0.0,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )

            # è§£ç å“åº”
            response_ids = generated_ids[0, len(inputs["input_ids"][0]):]
            raw_response = self.processor.decode(response_ids, skip_special_tokens=True)

            logger.info(f"åŸå§‹å“åº”: {raw_response}")

            # è§£æåæ ‡
            point_x, point_y = self._parse_coordinates(raw_response)

            processing_time = time.time() - start_time

            # æ„å»ºå“åº”
            elements = []
            actions = []

            if point_x != -1 and point_y != -1:
                # è·å–åŸå§‹å›¾åƒå°ºå¯¸
                original_size = image.size

                # è·å–resizeåçš„å›¾åƒå°ºå¯¸
                _, _, resized_height, resized_width = inputs['pixel_values'].shape

                # è®¡ç®—å½’ä¸€åŒ–åæ ‡
                norm_x = point_x / resized_width
                norm_y = point_y / resized_height

                # è½¬æ¢ä¸ºåŸå§‹å›¾åƒåæ ‡
                real_x = int(norm_x * original_size[0])
                real_y = int(norm_y * original_size[1])

                # åˆ›å»ºè¾¹ç•Œæ¡†
                bbox_size = 30  # è¾¹ç•Œæ¡†å¤§å°
                bbox = [
                    max(0, real_x - bbox_size // 2),
                    max(0, real_y - bbox_size // 2),
                    min(original_size[0], real_x + bbox_size // 2),
                    min(original_size[1], real_y + bbox_size // 2)
                ]

                elements.append({
                    "bbox": bbox,
                    "text": query,
                    "type": "ui_element",
                    "confidence": 0.95,  # UI-Ins-7Bé€šå¸¸ç»™å‡ºé«˜ç½®ä¿¡åº¦
                    "description": f"UI-Ins-7Bè¯†åˆ«çš„UIå…ƒç´ ï¼Œåæ ‡: ({real_x}, {real_y})",
                    "raw_coordinates": [point_x, point_y],
                    "normalized_coordinates": [norm_x, norm_y]
                })

                # ç”Ÿæˆæ“ä½œå»ºè®®
                if any(action in query.lower() for action in ["click", "ç‚¹å‡»", "tap", "press"]):
                    actions.append({
                        "type": "click",
                        "target": {"bbox": bbox},
                        "reason": f"UI-Ins-7Bæ ¹æ®æŒ‡ä»¤ '{query}' å®šä½å¹¶ç‚¹å‡»UIå…ƒç´ "
                    })

            return {
                "request_id": 1,
                "response": raw_response,
                "elements": elements,
                "actions": actions,
                "processing_time": processing_time,
                "device": str(self.model.device),
                "model_info": {
                    "model": "UI-Ins-7B",
                    "framework": "PyTorch",
                    "architecture": "Qwen2.5-VL",
                    "format": "GUI Agent"
                }
            }

        except Exception as e:
            logger.error(f"UI-Ins-7Bè¯†åˆ«å¤±è´¥: {e}")
            raise RuntimeError(f"UI-Ins-7Bè¯†åˆ«å¤±è´¥: {e}")

# FastAPIåº”ç”¨
app = FastAPI(title="UI-Ins-7B Recognition Service", version="1.0.0")

# å…¨å±€æœåŠ¡å®ä¾‹
ui_ins_service = UIIns7BService()

class RecognitionRequest(BaseModel):
    request_id: int
    image: str
    query: str = "è¯†åˆ«é¡µé¢ä¸­çš„å¯äº¤äº’å…ƒç´ "
    scope: str = "full"
    region: Optional[Dict[str, int]] = None
    parameters: Dict[str, Any] = {}

class RecognitionResponse(BaseModel):
    request_id: int
    response: str
    elements: List[Dict[str, Any]]
    actions: List[Dict[str, Any]]
    processing_time: float
    device: str
    model_info: Dict[str, str]

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ å¯åŠ¨UI-Ins-7Bè¯†åˆ«æœåŠ¡")
    logger.info("ğŸ¤– æ¨¡å‹: UI-Ins-7B (é˜¿é‡Œé€šä¹‰)")
    logger.info("ğŸ¯ åŠŸèƒ½: GUI Grounding & UI Element Recognition")
    logger.info("ğŸ“ æ ¼å¼: Official GUI Agent Format")

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­äº‹ä»¶"""
    logger.info("ğŸ”„ å…³é—­UI-Ins-7Bè¯†åˆ«æœåŠ¡")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "model_loaded": ui_ins_service.model_loaded,
        "model_path": ui_ins_service.model_path,
        "model": "UI-Ins-7B",
        "framework": "PyTorch",
        "version": "1.0.0",
        "dependencies": {
            "torch": True,
            "transformers": True,
            "fastapi": True
        }
    }

@app.post("/recognize", response_model=RecognitionResponse)
async def recognize_ui_elements(request: RecognitionRequest):
    """UIå…ƒç´ è¯†åˆ«ç«¯ç‚¹"""
    try:
        # è§£ç å›¾åƒ
        image = ui_ins_service._decode_image(request.image)

        # æ‰§è¡ŒUIè¯†åˆ«
        result = ui_ins_service.recognize_ui_elements(image, request.query)

        return RecognitionResponse(**result)

    except Exception as e:
        logger.error(f"è¯†åˆ«è¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="UI-Ins-7B Recognition Service")
    parser.add_argument("--host", default="0.0.0.0", help="æœåŠ¡ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8899, help="æœåŠ¡ç«¯å£")
    # é»˜è®¤ä½¿ç”¨ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„æ¨¡å‹ç›®å½•
    default_model_dir = os.environ.get("UI_INS_MODEL_PATH") or os.path.join(os.path.dirname(__file__), "models", "ui-ins-7b")
    parser.add_argument("--model-path", default=default_model_dir, help="æ¨¡å‹è·¯å¾„")
    args = parser.parse_args()

    # å…è®¸é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–æ¨¡å‹è·¯å¾„
    try:
        ui_ins_service.model_path = args.model_path
    except Exception:
        pass

    uvicorn.run(
        "ui_ins_server:app",
        host=args.host,
        port=args.port,
        reload=False,
        log_level="info"
    )

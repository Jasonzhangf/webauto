"""
WebAuto æµè§ˆå™¨æ¨¡å— - å®Œæ•´ç¤ºä¾‹é¡¹ç›®
ç½‘ç«™ä¿¡æ¯é‡‡é›†å™¨
"""

import json
import time
from datetime import datetime
from browser_interface import create_browser, stealth_mode

class WebsiteScraper:
    """ç½‘ç«™ä¿¡æ¯é‡‡é›†å™¨"""
    
    def __init__(self, headless=False, use_stealth=False):
        self.headless = headless
        self.use_stealth = use_stealth
        self.results = []
    
    def scrape_single_site(self, url: str) -> dict:
        """é‡‡é›†å•ä¸ªç½‘ç«™ä¿¡æ¯"""
        result = {
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'status': 'pending',
            'error': None
        }
        
        try:
            # é€‰æ‹©æµè§ˆå™¨æ¨¡å¼
            if self.use_stealth:
                browser_creator = stealth_mode
            else:
                browser_creator = create_browser
            
            with browser_creator(headless=self.headless) as browser:
                print(f"ğŸ” æ­£åœ¨è®¿é—®: {url}")
                page = browser.goto(url)
                
                # é‡‡é›†åŸºæœ¬ä¿¡æ¯
                result.update({
                    'title': page.title(),
                    'final_url': page.url(),
                    'status': 'success'
                })
                
                # é‡‡é›†é¡µé¢ä¿¡æ¯
                page_info = self._extract_page_info(page)
                result.update(page_info)
                
                # æˆªå›¾
                screenshot_name = f"screenshot_{url.replace('https://', '').replace('/', '_')}.png"
                page.screenshot(screenshot_name)
                result['screenshot'] = screenshot_name
                
                print(f"âœ… é‡‡é›†æˆåŠŸ: {result['title']}")
                
        except Exception as e:
            result.update({
                'status': 'failed',
                'error': str(e)
            })
            print(f"âŒ é‡‡é›†å¤±è´¥: {url} - {e}")
        
        return result
    
    def _extract_page_info(self, page) -> dict:
        """æå–é¡µé¢è¯¦ç»†ä¿¡æ¯"""
        info = {}
        
        try:
            # å°è¯•è·å–é¡µé¢æè¿°
            description = page.text_content('meta[name="description"]')
            if description:
                info['description'] = description.strip()
        except:
            pass
        
        try:
            # å°è¯•è·å–é¡µé¢å…³é”®è¯
            keywords = page.text_content('meta[name="keywords"]')
            if keywords:
                info['keywords'] = keywords.strip()
        except:
            pass
        
        try:
            # è·å–é¡µé¢è¯­è¨€
            lang = page.evaluate('document.documentElement.lang')
            if lang:
                info['language'] = lang
        except:
            pass
        
        try:
            # è·å–å­—ç¬¦ç¼–ç 
            charset = page.evaluate('document.characterSet')
            if charset:
                info['charset'] = charset
        except:
            pass
        
        try:
            # è·å–é¡µé¢å¤§å°
            size = page.evaluate('{width: document.body.scrollWidth, height: document.body.scrollHeight}')
            info['page_size'] = size
        except:
            pass
        
        return info
    
    def scrape_multiple_sites(self, urls: list, delay: int = 2) -> list:
        """é‡‡é›†å¤šä¸ªç½‘ç«™ä¿¡æ¯"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡é‡‡é›† {len(urls)} ä¸ªç½‘ç«™...")
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] é‡‡é›†: {url}")
            
            result = self.scrape_single_site(url)
            self.results.append(result)
            
            # å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            if i < len(urls):
                print(f"â³ ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)
        
        return self.results
    
    def save_results(self, filename: str = 'scraping_results.json'):
        """ä¿å­˜é‡‡é›†ç»“æœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def generate_report(self) -> str:
        """ç”Ÿæˆé‡‡é›†æŠ¥å‘Š"""
        total = len(self.results)
        successful = len([r for r in self.results if r['status'] == 'success'])
        failed = total - successful
        
        report = f"""
ğŸ“Š ç½‘ç«™ä¿¡æ¯é‡‡é›†æŠ¥å‘Š
====================
ğŸ“… é‡‡é›†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸŒ æ€»ç½‘ç«™æ•°: {total}
âœ… æˆåŠŸé‡‡é›†: {successful}
âŒ é‡‡é›†å¤±è´¥: {failed}
ğŸ“ˆ æˆåŠŸç‡: {(successful/total*100):.1f}%

ğŸ“‹ è¯¦ç»†ç»“æœ:
"""
        
        for i, result in enumerate(self.results, 1):
            status_emoji = "âœ…" if result['status'] == 'success' else "âŒ"
            title = result.get('title', 'æœªçŸ¥')
            url = result['url']
            
            report += f"\n{i}. {status_emoji} {title}\n   ğŸ“ {url}"
            
            if result['status'] == 'success':
                if 'description' in result:
                    report += f"\n   ğŸ“ {result['description'][:100]}..."
                if 'language' in result:
                    report += f"\n   ğŸŒ è¯­è¨€: {result['language']}"
                if 'charset' in result:
                    report += f"\n   ğŸ”¤ ç¼–ç : {result['charset']}"
            else:
                report += f"\n   âš ï¸  é”™è¯¯: {result['error']}"
            
            report += "\n"
        
        return report

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´é¡¹ç›®"""
    print("ğŸŒ WebAuto ç½‘ç«™ä¿¡æ¯é‡‡é›†å™¨")
    print("=" * 40)
    
    # é…ç½®é‡‡é›†å™¨
    scraper = WebsiteScraper(
        headless=False,    # æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢
        use_stealth=True   # ä½¿ç”¨éšåŒ¿æ¨¡å¼
    )
    
    # ç›®æ ‡ç½‘ç«™åˆ—è¡¨
    target_sites = [
        'https://www.baidu.com',
        'https://weibo.com',
        'https://www.zhihu.com',
        'https://github.com',
        'https://www.stackoverflow.com'
    ]
    
    try:
        # å¼€å§‹é‡‡é›†
        results = scraper.scrape_multiple_sites(target_sites, delay=2)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = scraper.generate_report()
        print(report)
        
        # ä¿å­˜ç»“æœ
        scraper.save_results('website_scraping_results.json')
        
        print("\nğŸ‰ é‡‡é›†ä»»åŠ¡å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­é‡‡é›†")
    except Exception as e:
        print(f"\nğŸ’¥ é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
